{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'missingno'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-09c7d7aed045>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmissingno\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmsno\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'missingno'"
     ]
    }
   ],
   "source": [
    "import pylab\n",
    "import calendar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from scipy import stats\n",
    "import missingno as msno\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "from os.path import join\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyData = pd.read_csv(f'{os.getenv(\"HOME\")}/aiffel/bike_sharing_demand/train.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyData.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyData[\"date\"] = dailyData.datetime.apply(lambda x : x.split()[0])\n",
    "dailyData[\"hour\"] = dailyData.datetime.apply(lambda x : x.split()[1].split(\":\")[0])\n",
    "dailyData[\"weekday\"] = dailyData.date.apply(lambda dateString : calendar.day_name[datetime.strptime(dateString,\"%Y-%m-%d\").weekday()])\n",
    "dailyData[\"month\"] = dailyData.date.apply(lambda dateString : calendar.month_name[datetime.strptime(dateString,\"%Y-%m-%d\").month])\n",
    "dailyData[\"season\"] = dailyData.season.map({1: \"Spring\", 2 : \"Summer\", 3 : \"Fall\", 4 :\"Winter\" })\n",
    "dailyData[\"weather\"] = dailyData.weather.map({1: \" Clear + Few clouds + Partly cloudy + Partly cloudy\",\\\n",
    "                                        2 : \" Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \", \\\n",
    "                                        3 : \" Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\", \\\n",
    "                                        4 :\" Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryVariableList = [\"hour\",\"weekday\",\"month\",\"season\",\"weather\",\"holiday\",\"workingday\"]\n",
    "for var in categoryVariableList:\n",
    "    dailyData[var] = dailyData[var].astype(\"category\")\n",
    "dailyData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyData  = dailyData.drop([\"datetime\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTypeDf = pd.DataFrame(dailyData.dtypes.value_counts()).reset_index().rename(columns={\"index\":\"variableType\",0:\"count\"})\n",
    "dataTypeDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots()\n",
    "# fig.set_size_inches(12,5)\n",
    "# sn.barplot(data=dataTypeDf,x=\"variableType\",y=\"count\",ax=ax)\n",
    "# ax.set(xlabel='variableTypeariable Type', ylabel='Count',title=\"Variables DataType Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(dailyData,figsize=(12,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2,ncols=2)\n",
    "fig.set_size_inches(12, 10)\n",
    "sn.boxplot(data=dailyData,y=\"count\",orient=\"v\",ax=axes[0][0])\n",
    "sn.boxplot(data=dailyData,y=\"count\",x=\"season\",orient=\"v\",ax=axes[0][1])\n",
    "sn.boxplot(data=dailyData,y=\"count\",x=\"hour\",orient=\"v\",ax=axes[1][0])\n",
    "sn.boxplot(data=dailyData,y=\"count\",x=\"workingday\",orient=\"v\",ax=axes[1][1])\n",
    "\n",
    "axes[0][0].set(ylabel='Count',title=\"Box Plot On Count\")\n",
    "axes[0][1].set(xlabel='Season', ylabel='Count',title=\"Box Plot On Count Across Season\")\n",
    "axes[1][0].set(xlabel='Hour Of The Day', ylabel='Count',title=\"Box Plot On Count Across Hour Of The Day\")\n",
    "axes[1][1].set(xlabel='Working Day', ylabel='Count',title=\"Box Plot On Count Across Working Day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyDataWithoutOutliers = dailyData[np.abs(dailyData[\"count\"]-dailyData[\"count\"].mean())<=(3*dailyData[\"count\"].std())] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Shape Of The Before Ouliers: \",dailyData.shape)\n",
    "print (\"Shape Of The After Ouliers: \",dailyDataWithoutOutliers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatt = dailyData[[\"temp\",\"atemp\",\"casual\",\"registered\",\"humidity\",\"windspeed\",\"count\"]].corr()\n",
    "mask = np.array(corrMatt)\n",
    "mask[np.tril_indices_from(mask)] = False\n",
    "fig,ax= plt.subplots()\n",
    "fig.set_size_inches(20,10)\n",
    "sn.heatmap(corrMatt, mask=mask,vmax=.8, square=True,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3) = plt.subplots(ncols=3)\n",
    "fig.set_size_inches(12, 5)\n",
    "sn.regplot(x=\"temp\", y=\"count\", data=dailyData,ax=ax1)\n",
    "sn.regplot(x=\"windspeed\", y=\"count\", data=dailyData,ax=ax2)\n",
    "sn.regplot(x=\"humidity\", y=\"count\", data=dailyData,ax=ax3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(ncols=2,nrows=2)\n",
    "fig.set_size_inches(12, 10)\n",
    "sn.distplot(dailyData[\"count\"],ax=axes[0][0])\n",
    "stats.probplot(dailyData[\"count\"], dist='norm', fit=True, plot=axes[0][1])\n",
    "sn.distplot(np.log(dailyDataWithoutOutliers[\"count\"]),ax=axes[1][0])\n",
    "stats.probplot(np.log1p(dailyDataWithoutOutliers[\"count\"]), dist='norm', fit=True, plot=axes[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3,ax4)= plt.subplots(nrows=4)\n",
    "fig.set_size_inches(12,20)\n",
    "sortOrder = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
    "hueOrder = [\"Sunday\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"]\n",
    "\n",
    "\n",
    "\n",
    "monthAggregated = pd.DataFrame(dailyData.groupby(\"month\")[\"count\"].mean()).reset_index()\n",
    "monthSorted = monthAggregated.sort_values(by=\"count\",ascending=False)\n",
    "sn.barplot(data=monthSorted,x=\"month\",y=\"count\",ax=ax1,order=sortOrder)\n",
    "ax1.set(xlabel='Month', ylabel='Avearage Count',title=\"Average Count By Month\")\n",
    "\n",
    "hourAggregated = pd.DataFrame(dailyData.groupby([\"hour\",\"season\"],sort=True)[\"count\"].mean()).reset_index()\n",
    "sn.pointplot(x=hourAggregated[\"hour\"], y=hourAggregated[\"count\"],hue=hourAggregated[\"season\"], data=hourAggregated, join=True,ax=ax2)\n",
    "ax2.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across Season\",label='big')\n",
    "\n",
    "hourAggregated = pd.DataFrame(dailyData.groupby([\"hour\",\"weekday\"],sort=True)[\"count\"].mean()).reset_index()\n",
    "sn.pointplot(x=hourAggregated[\"hour\"], y=hourAggregated[\"count\"],hue=hourAggregated[\"weekday\"],hue_order=hueOrder, data=hourAggregated, join=True,ax=ax3)\n",
    "ax3.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across Weekdays\",label='big')\n",
    "\n",
    "hourTransformed = pd.melt(dailyData[[\"hour\",\"casual\",\"registered\"]], id_vars=['hour'], value_vars=['casual', 'registered'])\n",
    "hourAggregated = pd.DataFrame(hourTransformed.groupby([\"hour\",\"variable\"],sort=True)[\"value\"].mean()).reset_index()\n",
    "sn.pointplot(x=hourAggregated[\"hour\"], y=hourAggregated[\"value\"],hue=hourAggregated[\"variable\"],hue_order=[\"casual\",\"registered\"], data=hourAggregated, join=True,ax=ax4)\n",
    "ax4.set(xlabel='Hour Of The Day', ylabel='Users Count',title=\"Average Users Count By Hour Of The Day Across User Type\",label='big')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling 0's In windspeed Using Random Forest ##\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46912607/128964248-a2acff80-cc46-4dc5-a037-a4f2de15ba51.png)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46912607/128964312-6bc2d568-ddb9-43f6-ba10-a7c98a7effb6.png)\n",
    "\n",
    "https://www.kaggle.com/c/bike-sharing-demand/discussion/10431\n",
    "\n",
    "\n",
    "데이터에는 1000 zero-windspeed 가 있다. \n",
    "\n",
    "1. windspeed 가 0 이다 \n",
    "\n",
    "2. windspeed 가 너무 낮아서 측정하기 힘들다 \n",
    "\n",
    "3. NAs다\n",
    "\n",
    "많은 사람들은 windspeed 없이 제출 했다고 하네요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain = pd.read_csv(f'{os.getenv(\"HOME\")}/aiffel/bike_sharing_demand/train.csv')\n",
    "dataTest = pd.read_csv(f'{os.getenv(\"HOME\")}/aiffel/bike_sharing_demand/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataTrain.append(dataTest)\n",
    "data.reset_index(inplace=True)\n",
    "data.drop('index',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['windspeed'] == 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"windspeed\"]== 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws=data.windspeed.unique().tolist()\n",
    "ws.sort()\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 0 ~ 6 사이의 값이 없다. 이거는 너무 낮아서 측정이 안되는거 일수도 \n",
    "\n",
    "2. 모든 플로트 값들은 정수값의 1% 범위에 있다. \n",
    "\n",
    "3. 바람 측정치 중 9, 10 등은 없다 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering\n",
    "\n",
    "As we see from the above results, the columns \"season\",\"holiday\",\"workingday\" and \"weather\" should be of \"categorical\" data type.But the current data type is \"int\" for those columns. Let us transform the dataset in the following ways so that we can get started up with our EDA\n",
    "\n",
    "- Create new columns \"date,\"hour\",\"weekDay\",\"month\" from \"datetime\" column.\n",
    "\n",
    "- Coerce the datatype of \"season\",\"holiday\",\"workingday\" and weather to category.\n",
    "\n",
    "- Drop the datetime column as we already extracted useful features from it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"date\"] = data.datetime.apply(lambda x : x.split()[0])\n",
    "data[\"hour\"] = data.datetime.apply(lambda x : x.split()[1].split(\":\")[0]).astype(\"int\")\n",
    "data[\"year\"] = data.datetime.apply(lambda x : x.split()[0].split(\"-\")[0])\n",
    "data[\"weekday\"] = data.date.apply(lambda dateString : datetime.strptime(dateString,\"%Y-%m-%d\").weekday())\n",
    "\n",
    "data[\"month\"] = data.date.apply(lambda dateString : datetime.strptime(dateString,\"%Y-%m-%d\").month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Model To Predict 0's In Windspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "dataWind0 = data[data[\"windspeed\"]==0]\n",
    "dataWindNot0 = data[data[\"windspeed\"]!=0]\n",
    "rfModel_wind = RandomForestRegressor()\n",
    "windColumns = [\"season\",\"weather\",\"humidity\",\"month\",\"temp\",\"year\",\"atemp\"]\n",
    "rfModel_wind.fit(dataWindNot0[windColumns], dataWindNot0[\"windspeed\"])\n",
    "\n",
    "wind0Values = rfModel_wind.predict(X= dataWind0[windColumns])\n",
    "dataWind0[\"windspeed\"] = wind0Values\n",
    "data = dataWindNot0.append(dataWind0)\n",
    "data.reset_index(inplace=True)\n",
    "data.drop('index',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalFeatureNames = [\"season\",\"holiday\",\"workingday\",\"weather\",\"weekday\",\"month\",\"year\",\"hour\"]\n",
    "numericalFeatureNames = [\"temp\",\"humidity\",\"windspeed\",\"atemp\"]\n",
    "dropFeatures = ['casual',\"count\",\"datetime\",\"date\",\"registered\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in categoricalFeatureNames:\n",
    "    data[var] = data[var].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain = data[pd.notnull(data['count'])].sort_values(by=[\"datetime\"])\n",
    "dataTest = data[~pd.notnull(data['count'])].sort_values(by=[\"datetime\"])\n",
    "datetimecol = dataTest[\"datetime\"]\n",
    "yLabels = dataTrain[\"count\"]\n",
    "yLablesRegistered = dataTrain[\"registered\"]\n",
    "yLablesCasual = dataTrain[\"casual\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain  = dataTrain.drop(dropFeatures,axis=1)\n",
    "dataTest  = dataTest.drop(dropFeatures,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws=dataTrain.windspeed.unique().tolist()\n",
    "ws.sort()\n",
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yLabelsLog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수정전\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46912607/128968700-8204db80-1cbc-4fb2-b303-42852227c44c.png)\n",
    "\n",
    "수정후\n",
    "![image](https://user-images.githubusercontent.com/46912607/128968305-c613abcd-dfd9-456d-84a9-fb6952edce84.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y, y_,convertExp=True):\n",
    "    if convertExp:\n",
    "        y = np.exp(y),\n",
    "        y_ = np.exp(y_)\n",
    "    log1 = np.nan_to_num(np.array([np.log(v + 1) for v in y]))\n",
    "    log2 = np.nan_to_num(np.array([np.log(v + 1) for v in y_]))\n",
    "    calc = (log1 - log2) ** 2\n",
    "    return np.sqrt(np.mean(calc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Initialize logistic regression model\n",
    "lModel = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "yLabelsLog = np.log1p(yLabels)\n",
    "lModel.fit(X = dataTrain,y = yLabelsLog)\n",
    "\n",
    "# Make predictions\n",
    "preds = lModel.predict(X= dataTrain)\n",
    "print (\"RMSLE Value For Linear Regression: \",rmsle(np.exp(yLabelsLog),np.exp(preds),False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_m_ = Ridge()\n",
    "ridge_params_ = { 'max_iter':[3000],'alpha':[0.1, 1, 2, 3, 4, 10, 30,100,200,300,400,800,900,1000]}\n",
    "rmsle_scorer = metrics.make_scorer(rmsle, greater_is_better=False)\n",
    "grid_ridge_m = GridSearchCV( ridge_m_,\n",
    "                          ridge_params_,\n",
    "                          scoring = rmsle_scorer,\n",
    "                          cv=5)\n",
    "yLabelsLog = np.log1p(yLabels)\n",
    "grid_ridge_m.fit( dataTrain, yLabelsLog )\n",
    "preds = grid_ridge_m.predict(X= dataTrain)\n",
    "print (grid_ridge_m.best_params_)\n",
    "print (\"RMSLE Value For Ridge Regression: \",rmsle(np.exp(yLabelsLog),np.exp(preds),False))\n",
    "\n",
    "fig,ax= plt.subplots()\n",
    "fig.set_size_inches(12,5)\n",
    "df = pd.DataFrame(grid_ridge_m.cv_results_)\n",
    "df[\"alpha\"] = df[\"params\"].apply(lambda x:x[\"alpha\"])\n",
    "df[\"rmsle\"] = df[\"mean_test_score\"].apply(lambda x:-x)\n",
    "sn.pointplot(data=df,x=\"alpha\",y=\"rmsle\",ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(grid_ridge_m.grid_scores_) \n",
    "# 1번째 오류 발생 df = pd.DataFrame(grid_ridge_m.cv_results_) \n",
    "#df[\"alpha\"] = df[\"parameters\"].apply(lambda x:x[\"alpha\"]) \n",
    "# 2번째 오류 발생 df[\"alpha\"] = df[\"params\"].apply(lambda x:x[\"alpha\"]) \n",
    "#df[\"rmsle\"] = df[\"mean_validation_score\"].apply(lambda x:-x)\n",
    "# 3번째 오류 발생 df[\"rmsle\"] = df[\"mean_test_score\"].apply(lambda x:-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_m_ = Lasso()\n",
    "\n",
    "alpha  = 1/np.array([0.1, 1, 2, 3, 4, 10, 30,100,200,300,400,800,900,1000])\n",
    "lasso_params_ = { 'max_iter':[3000],'alpha':alpha}\n",
    "\n",
    "grid_lasso_m = GridSearchCV( lasso_m_,lasso_params_,scoring = rmsle_scorer,cv=5)\n",
    "yLabelsLog = np.log1p(yLabels)\n",
    "grid_lasso_m.fit( dataTrain, yLabelsLog )\n",
    "preds = grid_lasso_m.predict(X= dataTrain)\n",
    "print (grid_lasso_m.best_params_)\n",
    "print (\"RMSLE Value For Lasso Regression: \",rmsle(np.exp(yLabelsLog),np.exp(preds),False))\n",
    "\n",
    "fig,ax= plt.subplots()\n",
    "fig.set_size_inches(12,5)\n",
    "df = pd.DataFrame(grid_lasso_m.cv_results_)\n",
    "df[\"alpha\"] = df[\"params\"].apply(lambda x:x[\"alpha\"])\n",
    "df[\"rmsle\"] = df[\"mean_test_score\"].apply(lambda x:-x)\n",
    "sn.pointplot(data=df,x=\"alpha\",y=\"rmsle\",ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfModel = RandomForestRegressor(n_estimators=100)\n",
    "yLabelsLog = np.log1p(yLabels)\n",
    "rfModel.fit(dataTrain,yLabelsLog)\n",
    "preds = rfModel.predict(X= dataTrain)\n",
    "print (\"RMSLE Value For Random Forest: \",rmsle(np.exp(yLabelsLog),np.exp(preds),False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbm = GradientBoostingRegressor(n_estimators=4000,alpha=0.01); ### Test 0.41\n",
    "yLabelsLog = np.log1p(yLabels)\n",
    "gbm.fit(dataTrain,yLabelsLog)\n",
    "preds = gbm.predict(X= dataTrain)\n",
    "print (\"RMSLE Value For Gradient Boost: \",rmsle(np.exp(yLabelsLog),np.exp(preds),False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsTest = gbm.predict(X= dataTest)\n",
    "fig,(ax1,ax2)= plt.subplots(ncols=2)\n",
    "fig.set_size_inches(12,5)\n",
    "sn.distplot(yLabels,ax=ax1,bins=50)\n",
    "sn.distplot(np.exp(predsTest),ax=ax2,bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"datetime\": datetimecol,\n",
    "        \"count\": [max(0, x) for x in np.exp(predsTest)]\n",
    "    })\n",
    "submission.to_csv('bike_predictions_gbm_separate_without_fe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Initialize logistic regression model\n",
    "lModel = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "yLabelsLog = np.log1p(yLabels)\n",
    "lModel.fit(X = dataTrain,y = yLabelsLog)\n",
    "\n",
    "# Make predictions\n",
    "preds = lModel.predict(X= dataTrain)\n",
    "print (\"RMSLE Value For Linear Regression: \",rmsle(np.exp(yLabelsLog),np.exp(preds),False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_m_ = Ridge()\n",
    "ridge_params_ = { 'max_iter':[3000],'alpha':[0.1, 1, 2, 3, 4, 10, 30,100,200,300,400,800,900,1000]}\n",
    "rmsle_scorer = metrics.make_scorer(rmsle, greater_is_better=False)\n",
    "grid_ridge_m = GridSearchCV( ridge_m_,\n",
    "                          ridge_params_,\n",
    "                          scoring = rmsle_scorer,\n",
    "                          cv=5)\n",
    "yLabelsLog = np.log1p(yLabels)\n",
    "grid_ridge_m.fit( dataTrain, yLabelsLog )\n",
    "preds = grid_ridge_m.predict(X= dataTrain)\n",
    "print (grid_ridge_m.best_params_)\n",
    "print (\"RMSLE Value For Ridge Regression: \",rmsle(np.exp(yLabelsLog),np.exp(preds),False))\n",
    "\n",
    "fig,ax= plt.subplots()\n",
    "fig.set_size_inches(12,5)\n",
    "df = pd.DataFrame(grid_ridge_m.grid_scores_)\n",
    "df[\"alpha\"] = df[\"parameters\"].apply(lambda x:x[\"alpha\"])\n",
    "df[\"rmsle\"] = df[\"mean_validation_score\"].apply(lambda x:-x)\n",
    "sn.pointplot(data=df,x=\"alpha\",y=\"rmsle\",ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_m_ = Lasso()\n",
    "\n",
    "alpha  = 1/np.array([0.1, 1, 2, 3, 4, 10, 30,100,200,300,400,800,900,1000])\n",
    "lasso_params_ = { 'max_iter':[3000],'alpha':alpha}\n",
    "\n",
    "grid_lasso_m = GridSearchCV( lasso_m_,lasso_params_,scoring = rmsle_scorer,cv=5)\n",
    "yLabelsLog = np.log1p(yLabels)\n",
    "grid_lasso_m.fit( dataTrain, yLabelsLog )\n",
    "preds = grid_lasso_m.predict(X= dataTrain)\n",
    "print (grid_lasso_m.best_params_)\n",
    "print (\"RMSLE Value For Lasso Regression: \",rmsle(np.exp(yLabelsLog),np.exp(preds),False))\n",
    "\n",
    "fig,ax= plt.subplots()\n",
    "fig.set_size_inches(12,5)\n",
    "df = pd.DataFrame(grid_lasso_m.grid_scores_)\n",
    "df[\"alpha\"] = df[\"parameters\"].apply(lambda x:x[\"alpha\"])\n",
    "df[\"rmsle\"] = df[\"mean_validation_score\"].apply(lambda x:-x)\n",
    "sn.pointplot(data=df,x=\"alpha\",y=\"rmsle\",ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfModel = RandomForestRegressor(n_estimators=100)\n",
    "yLabelsLog = np.log1p(yLabels)\n",
    "rfModel.fit(dataTrain,yLabelsLog)\n",
    "preds = rfModel.predict(X= dataTrain)\n",
    "print (\"RMSLE Value For Random Forest: \",rmsle(np.exp(yLabelsLog),np.exp(preds),False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbm = GradientBoostingRegressor(n_estimators=4000,alpha=0.01); ### Test 0.41\n",
    "yLabelsLog = np.log1p(yLabels)\n",
    "gbm.fit(dataTrain,yLabelsLog)\n",
    "preds = gbm.predict(X= dataTrain)\n",
    "print (\"RMSLE Value For Gradient Boost: \",rmsle(np.exp(yLabelsLog),np.exp(preds),False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsTest = gbm.predict(X= dataTest)\n",
    "fig,(ax1,ax2)= plt.subplots(ncols=2)\n",
    "fig.set_size_inches(12,5)\n",
    "sn.distplot(yLabels,ax=ax1,bins=50)\n",
    "sn.distplot(np.exp(predsTest),ax=ax2,bins=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
